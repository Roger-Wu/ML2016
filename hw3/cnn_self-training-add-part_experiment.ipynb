{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "METHOD = 'cnn_self-training-add-part_experiment'\n",
    "\n",
    "LABELLED_DATA_FILE = 'data/all_label.p'\n",
    "UNLABELED_DATA_FILE = 'data/all_unlabel.p'\n",
    "TEST_DATA_FILE = 'data/test.p'\n",
    "\n",
    "OUTPUT_FOLDER = 'output/'\n",
    "MODEL_FOLDER = 'model/' + METHOD + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(MODEL_FOLDER):\n",
    "    os.makedirs(MODEL_FOLDER)\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelled_data = np.array(pickle.load(open(LABELLED_DATA_FILE, 'rb')))\n",
    "unlabeled_data = np.array(pickle.load(open(UNLABELED_DATA_FILE, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "img_rows, img_cols, img_channels = 32, 32, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate x, y from data\n",
    "\n",
    "# reshape labelled data to (5000, 3, 32, 32)\n",
    "X_train_label = labelled_data.reshape((5000, img_channels, img_rows, img_cols)).astype('float32') / 255\n",
    "\n",
    "# reshape unlabeled data to (45000, 3, 32, 32)\n",
    "X_train_unlabel = unlabeled_data.reshape((unlabeled_data.shape[0], img_channels, img_rows, img_cols)).astype('float32') / 255\n",
    "\n",
    "y_train_label_class = np.array([classIdx for classIdx in range(len(labelled_data)) for i in range(len(labelled_data[classIdx]))])\n",
    "Y_train_label = np_utils.to_categorical(y_train_label_class, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: remove\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = cifar10.load_data()\n",
    "\n",
    "# X_train_reshaped = np.array([[img[:, :, 0], img[:, :, 1], img[:, :, 2]] for img in X_train]) / 255.0\n",
    "X_test_cifar = np.array([[img[:, :, 0], img[:, :, 1], img[:, :, 2]] for img in X_test_cifar]) / 255.0\n",
    "Y_test_cifar = np_utils.to_categorical(y_test_cifar.flatten(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJrElEQVR4nCXTW29cx2EA4LnPnDm3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\noyQO/gjApWe1WMMl7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F89DC658E80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize\n",
    "from scipy.misc import toimage\n",
    "toimage(X_train_unlabel[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def certainty(prob_arr):  # higher is more certain\n",
    "    return -entropy(prob_arr)\n",
    "\n",
    "def uncertainty(prob_arr):  # lower is better. for descending-ordered sorting\n",
    "    return entropy(prob_arr)\n",
    "\n",
    "#     sorted_arr = np.sort(prob_arr)\n",
    "#     return sorted_arr[-1] / sorted_arr[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first model\n",
    "batch_size = 64\n",
    "nb_epoch = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', input_shape=X_train_label.shape[1:], dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering=\"th\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(Convolution2D(64, 3, 3))\n",
    "# # model.add(Activation('relu'))\n",
    "# model.add(AveragePooling2D(pool_size=(2, 2), dim_ordering=\"th\", border_mode='same'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), dim_ordering=\"th\", border_mode='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())  # n * 8 * 8, for 64 -> 4096\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_label, Y_train_label,\n",
    "    batch_size=batch_size,\n",
    "    nb_epoch=nb_epoch,  # nb_epoch,\n",
    "    validation_data=(X_test_cifar, Y_test_cifar),\n",
    "    shuffle=True)\n",
    "\n",
    "model_1 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = (METHOD\n",
    "    + '_first-model'\n",
    "    + '_filters-64-48-512'\n",
    "#     + '_add-0.25'\n",
    "    + '_epo-100' # + str(nb_epoch)\n",
    "    + '_0.39' # + str(val_acc)[:6]\n",
    ")\n",
    "model.save(MODEL_FOLDER + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = 'model_second_6464MP6464APS512256128_extr45_epo23_acc9030_5436'\n",
    "model = load_model('model/cnn_self-training-add-part_experiment/' + model_name)\n",
    "model_1 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cnn_self-training-add-part_experiment_second-model_filters-64-64-MP-48-MP-512_add-0.4_epo-70_acc0.8823_0.4868'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 6s     \n",
      "45000/45000 [==============================] - 6s     \n"
     ]
    }
   ],
   "source": [
    "# use the model to label unlabeled data\n",
    "Y_train_unlabel_proba = model.predict_proba(X_train_unlabel)  # same as model.predict\n",
    "Y_train_unlabel_uncertainty = np.apply_along_axis(uncertainty, 1, Y_train_unlabel_proba)\n",
    "\n",
    "Y_train_unlabel_class = model.predict_classes(X_train_unlabel)\n",
    "Y_train_unlabel = np_utils.to_categorical(Y_train_unlabel_class, nb_classes)\n",
    "\n",
    "# sort unlabeled data by uncertainty\n",
    "sorted_idxs = Y_train_unlabel_uncertainty.argsort()\n",
    "X_train_unlabel_sorted = X_train_unlabel[sorted_idxs]\n",
    "Y_train_unlabel_sorted = Y_train_unlabel[sorted_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_ratio = 0.45\n",
    "\n",
    "# extract high-certainty unlabeled data\n",
    "nb_extract = int(X_train_unlabel.shape[0] * extract_ratio)\n",
    "X_train_unlabel_certain, X_train_unlabel_uncertain = X_train_unlabel_sorted[:nb_extract], X_train_unlabel_sorted[nb_extract:]\n",
    "Y_train_unlabel_certain, Y_train_unlabel_uncertain = Y_train_unlabel_sorted[:nb_extract], Y_train_unlabel_sorted[nb_extract:]\n",
    "\n",
    "# update labelled dataset and unlabeled dataset\n",
    "X_train_label_pool = np.concatenate((X_train_label, X_train_unlabel_certain))\n",
    "Y_train_label_pool = np.concatenate((Y_train_label, Y_train_unlabel_certain))\n",
    "X_train_unlabel_pool = X_train_unlabel_uncertain\n",
    "Y_train_unlabel_pool = Y_train_unlabel_uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# second model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same', input_shape=X_train_label.shape[1:], dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering=\"th\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(Convolution2D(64, 3, 3))\n",
    "# # model.add(Activation('relu'))\n",
    "# model.add(AveragePooling2D(pool_size=(2, 2), dim_ordering=\"th\", border_mode='same'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), dim_ordering=\"th\", border_mode='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())  # n * 8 * 8, for 64 -> 4096\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25250 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "25250/25250 [==============================] - 25s - loss: 0.1663 - acc: 0.9503 - val_loss: 2.0330 - val_acc: 0.5590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8931961f98>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_epoch = int(500000 / X_train_label_pool.shape[0])\n",
    "\n",
    "model.fit(X_train_label_pool, Y_train_label_pool,\n",
    "    batch_size=batch_size,\n",
    "    nb_epoch=1,  # nb_epoch,\n",
    "    validation_data=(X_test_cifar, Y_test_cifar),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_second_6464MP6464APS512256128_extr45_epo23_acc8944_5280 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_name = (METHOD\n",
    "#     + '_second-model'\n",
    "#     + '_filters-64-64-MP-64-64--512'\n",
    "#     + '_add-0.4'\n",
    "#     + '_epo-70' # + str(nb_epoch)\n",
    "#     + '_acc0.8823_0.4868' # + str(val_acc)[:6]\n",
    "# )\n",
    "model_name = \"model_third_128128MP128128APS1024256128_extr45_epo30_acc9457_5590\"\n",
    "model.save(MODEL_FOLDER + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = 'cnn_self-training-add-part_experiment_second-model_filters-64-64-MP-48-MP-512_add-0.4_epo-70_acc0.8823_0.4868'\n",
    "model = load_model('model/cnn_self-training-add-part_experiment/' + model_name)\n",
    "\n",
    "model_2 = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Third model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 6s     \n",
      "44992/45000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# use the model to label unlabeled data\n",
    "Y_train_unlabel_proba = model.predict_proba(X_train_unlabel)  # same as model.predict\n",
    "Y_train_unlabel_uncertainty = np.apply_along_axis(uncertainty, 1, Y_train_unlabel_proba)\n",
    "\n",
    "Y_train_unlabel_class = model.predict_classes(X_train_unlabel)\n",
    "Y_train_unlabel = np_utils.to_categorical(Y_train_unlabel_class, nb_classes)\n",
    "\n",
    "# sort unlabeled data by uncertainty\n",
    "sorted_idxs = Y_train_unlabel_uncertainty.argsort()\n",
    "X_train_unlabel_sorted = X_train_unlabel[sorted_idxs]\n",
    "Y_train_unlabel_sorted = Y_train_unlabel[sorted_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_ratio = 0.5\n",
    "\n",
    "# extract high-certainty unlabeled data\n",
    "nb_extract = int(X_train_unlabel.shape[0] * extract_ratio)\n",
    "X_train_unlabel_certain, X_train_unlabel_uncertain = X_train_unlabel_sorted[:nb_extract], X_train_unlabel_sorted[nb_extract:]\n",
    "Y_train_unlabel_certain, Y_train_unlabel_uncertain = Y_train_unlabel_sorted[:nb_extract], Y_train_unlabel_sorted[nb_extract:]\n",
    "\n",
    "# update labelled dataset and unlabeled dataset\n",
    "X_train_label_pool = np.concatenate((X_train_label, X_train_unlabel_certain))\n",
    "Y_train_label_pool = np.concatenate((Y_train_label, Y_train_unlabel_certain))\n",
    "X_train_unlabel_pool = X_train_unlabel_uncertain\n",
    "Y_train_unlabel_pool = Y_train_unlabel_uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# third model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering=\"th\", input_shape=X_train_label.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering=\"th\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Convolution2D(32, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(Convolution2D(48, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering=\"th\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27500 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "27500/27500 [==============================] - 13s - loss: 0.2040 - acc: 0.9295 - val_loss: 2.3113 - val_acc: 0.4210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcc5639ef0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_epoch = int(500000 / X_train_label_pool.shape[0])\n",
    "\n",
    "model.fit(X_train_label_pool, Y_train_label_pool,\n",
    "    batch_size=batch_size,\n",
    "    nb_epoch=1,  # nb_epoch,\n",
    "    validation_data=(X_test_cifar, Y_test_cifar),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = (METHOD\n",
    "    + '_third-model'\n",
    "    + '_filters-64-64-MP-48-MP-512'\n",
    "    + '_add-0.5'\n",
    "    + '_epo-50' # + str(nb_epoch)\n",
    "    + '_acc0.9203_0.48' # + str(val_acc)[:6]\n",
    ")\n",
    "model.save(MODEL_FOLDER + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the model to label unlabeled data\n",
    "Y_train_unlabel_proba = model.predict_proba(X_train_unlabel_pool)  # same as model.predict\n",
    "Y_train_unlabel_uncertainty = np.apply_along_axis(uncertainty, 1, Y_train_unlabel_proba)\n",
    "\n",
    "Y_train_unlabel_class = model.predict_classes(X_train_unlabel_pool, batch_size=10000)\n",
    "Y_train_unlabel = np_utils.to_categorical(Y_train_unlabel_class, nb_classes)\n",
    "\n",
    "# sort unlabeled data by uncertainty\n",
    "sorted_idxs = Y_train_unlabel_uncertainty.argsort()\n",
    "X_train_unlabel_sorted = X_train_unlabel_pool[sorted_idxs]\n",
    "Y_train_unlabel_sorted = Y_train_unlabel_pool[sorted_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_ratio = 0.2\n",
    "\n",
    "# extract high-certainty unlabeled data\n",
    "nb_extract = int(X_train_unlabel.shape[0] * extract_ratio)\n",
    "X_train_unlabel_certain, X_train_unlabel_uncertain = X_train_unlabel_sorted[:nb_extract], X_train_unlabel_sorted[nb_extract:]\n",
    "Y_train_unlabel_certain, Y_train_unlabel_uncertain = Y_train_unlabel_sorted[:nb_extract], Y_train_unlabel_sorted[nb_extract:]\n",
    "\n",
    "# update labelled dataset and unlabeled dataset\n",
    "X_train_label_pool_2 = np.concatenate((X_train_label, X_train_unlabel_certain))\n",
    "Y_train_label_pool_2 = np.concatenate((Y_train_label, Y_train_unlabel_certain))\n",
    "X_train_unlabel_pool_2 = X_train_unlabel_uncertain\n",
    "Y_train_unlabel_pool_2 = Y_train_unlabel_uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set model\n",
    "\n",
    "def reset_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering=\"th\", input_shape=X_train_label.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(Convolution2D(32, 3, 3))\n",
    "#     model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode='same', dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(Convolution2D(64, 3, 3))\n",
    "#     model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode='same', dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# avoid change to the original labelled data\n",
    "X_train_ori_label = np.copy(X_train_label)\n",
    "Y_train_ori_label = np.copy(Y_train_label)\n",
    "X_train_ori_unlabel = np.copy(X_train_unlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_ratio = 0.25\n",
    "nb_extract_rounds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract high-certainty labelled-unlabeled data and add to labelled dataset\n",
    "\n",
    "# TODO: remove (X_test_reshaped, Y_test_categ),\n",
    "\n",
    "for extract_round in range(nb_extract_rounds):\n",
    "    print('extract_round:', extract_round+1)\n",
    "    # reset model\n",
    "    model = reset_model()\n",
    "    \n",
    "    # train model\n",
    "    model.fit(X_train_label, Y_train_label,\n",
    "        batch_size=batch_size,\n",
    "        nb_epoch=nb_epoch,  # nb_epoch,\n",
    "        validation_data=(X_test_reshaped, Y_test_categ),\n",
    "        shuffle=True)\n",
    "\n",
    "    # use the model to label unlabeled data\n",
    "    Y_train_unlabel_proba = model.predict_proba(X_train_unlabel, batch_size=10000)  # same as model.predict\n",
    "    Y_train_unlabel_uncertainty = np.apply_along_axis(uncertainty, 1, Y_train_unlabel_proba)\n",
    "    \n",
    "    Y_train_unlabel_class = model.predict_classes(X_train_unlabel, batch_size=10000)\n",
    "    Y_train_unlabel = np_utils.to_categorical(Y_train_unlabel_class, nb_classes)\n",
    "\n",
    "    # sort unlabeled data by uncertainty\n",
    "    sorted_idxs = Y_train_unlabel_uncertainty.argsort()\n",
    "    X_train_unlabel = X_train_unlabel[sorted_idxs]\n",
    "    Y_train_unlabel = Y_train_unlabel[sorted_idxs]\n",
    "    \n",
    "    # extract high-certainty unlabeled data\n",
    "    nb_extract = int(X_train_unlabel.shape[0] * extract_ratio)\n",
    "    X_train_unlabel_certain, X_train_unlabel_uncertain = X_train_unlabel[:nb_extract], X_train_unlabel[nb_extract:]\n",
    "    Y_train_unlabel_certain, Y_train_unlabel_uncertain = Y_train_unlabel[:nb_extract], Y_train_unlabel[nb_extract:]\n",
    "\n",
    "    # update labelled dataset and unlabeled dataset\n",
    "    X_train_label = np.concatenate((X_train_label, X_train_unlabel_certain))\n",
    "    Y_train_label = np.concatenate((Y_train_label, Y_train_unlabel_certain))\n",
    "    X_train_unlabel = X_train_unlabel_uncertain\n",
    "    Y_train_unlabel = Y_train_unlabel_uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set model\n",
    "\n",
    "def reset_model_2():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering=\"th\", input_shape=X_train_label.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(Convolution2D(64, 3, 3))\n",
    "#     model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode='same', dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering=\"th\"))\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(Convolution2D(64, 3, 3))\n",
    "#     model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode='same', dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_bak = model\n",
    "\n",
    "model = reset_model_2()\n",
    "\n",
    "# TODO: remove (X_test_reshaped, Y_test_categ),\n",
    "\n",
    "model.fit(X_train_label, Y_train_label,\n",
    "    batch_size=batch_size,\n",
    "    nb_epoch=100,  # nb_epoch,\n",
    "    validation_data=(X_test_reshaped, Y_test_categ),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train_label_class_predict = model.predict_classes(X_train_ori_label, batch_size=5000)\n",
    "val_acc = np.sum(Y_train_label_class_predict == y_train_label_class) / float(len(y_train_label_class))\n",
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = (METHOD\n",
    "    + '_filters-64w3d0.5-32w3d0.5-512'\n",
    "#     + '_add-0.25'\n",
    "    + '_epo-100' # + str(nb_epoch)\n",
    "    + '_0.37' # + str(val_acc)[:6]\n",
    ")\n",
    "model.save(MODEL_FOLDER + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = pickle.load(open(TEST_DATA_FILE, 'rb'))  # dict\n",
    "X_test = np.array(test_data['data']).reshape((10000, 3, 32, 32)).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s     \n"
     ]
    }
   ],
   "source": [
    "result = model.predict_classes(X_test)\n",
    "csv_content = list(zip(test_data['ID'], result.tolist()))\n",
    "np.savetxt(OUTPUT_FOLDER + model_name + \".csv\", csv_content, fmt=\"%i\", header=\"ID,class\", comments=\"\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cnn_self-training-add-part_experiment_second-model_filters-64-64-MP-48-MP-512_add-0.4_epo-70_acc0.8823_0.4868'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train_reshaped = np.array([[img[:, :, 0], img[:, :, 1], img[:, :, 2]] for img in X_train])\n",
    "X_test_reshaped = np.array([[img[:, :, 0], img[:, :, 1], img[:, :, 2]] for img in X_test]) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48680000000000001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_class_predict = model.predict_classes(X_test_cifar)\n",
    "np.sum(Y_test_class_predict == y_test_cifar.flatten()) / len(y_test_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train_class_predict = model.predict_classes(X_train_reshaped, batch_size=10000)\n",
    "np.sum(Y_train_class_predict == y_train.flatten()) / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toimage(X_train_reshaped[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train_class_predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toimage(X_train_reshaped[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
