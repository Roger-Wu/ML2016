{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "METHOD = 'cnn_self-training'\n",
    "\n",
    "LABELLED_DATA_FILE = 'data/all_label.p'\n",
    "UNLABELED_DATA_FILE = 'data/all_unlabel.p'\n",
    "TEST_DATA_FILE = 'data/test.p'\n",
    "\n",
    "OUTPUT_FOLDER = 'output/'\n",
    "MODEL_FOLDER = 'model/' + METHOD + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(MODEL_FOLDER):\n",
    "    os.makedirs(MODEL_FOLDER)\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelled_data = np.array(pickle.load(open(LABELLED_DATA_FILE, 'rb')))\n",
    "unlabeled_data = np.array(pickle.load(open(UNLABELED_DATA_FILE, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "img_rows, img_cols, img_channels = 32, 32, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate x, y from data\n",
    "\n",
    "# reshape labelled data to (5000, 3, 32, 32)\n",
    "X_train_label = labelled_data.reshape((5000, img_channels, img_rows, img_cols)).astype('float32') / 255\n",
    "\n",
    "# reshape unlabeled data to (45000, 3, 32, 32)\n",
    "nb_unlabeled_data = unlabeled_data.shape[0]\n",
    "X_train_unlabel = unlabeled_data.reshape((nb_unlabeled_data, img_channels, img_rows, img_cols)).astype('float32') / 255\n",
    "\n",
    "y_train_label_class = np.array([classIdx for classIdx in range(len(labelled_data)) for i in range(len(labelled_data[classIdx]))])\n",
    "Y_train_label = np_utils.to_categorical(y_train_label_class, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJrElEQVR4nCXTW29cx2EA4LnPnDm3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\noyQO/gjApWe1WMMl7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE022DB0E80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize\n",
    "from scipy.misc import toimage\n",
    "toimage(X_train_unlabel[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "\n",
    "# nb_filters = [32, 64]\n",
    "# nb_nodes = [512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(64, 5, 5, border_mode='same', input_shape=X_train_label.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Convolution2D(64, 3, 3))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering=\"th\"))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Convolution2D(64, 3, 3))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering=\"th\"))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512))  # 32 * \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum (how original).\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 0s - loss: 2.1713 - acc: 0.1728     \n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.9781 - acc: 0.2792     \n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.8355 - acc: 0.3242     \n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.7242 - acc: 0.3810     \n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.6436 - acc: 0.4062     \n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.5714 - acc: 0.4254     \n",
      "Epoch 7/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.5348 - acc: 0.4468     \n",
      "Epoch 8/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.5004 - acc: 0.4570     \n",
      "Epoch 9/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.4591 - acc: 0.4650     \n",
      "Epoch 10/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.4155 - acc: 0.4916     \n",
      "Epoch 11/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.3772 - acc: 0.4958     \n",
      "Epoch 12/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.3463 - acc: 0.5158     \n",
      "Epoch 13/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.2985 - acc: 0.5286     \n",
      "Epoch 14/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.2534 - acc: 0.5498     \n",
      "Epoch 15/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.1940 - acc: 0.5728     \n",
      "Epoch 16/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.1691 - acc: 0.5824     \n",
      "Epoch 17/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.1277 - acc: 0.5908     \n",
      "Epoch 18/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.0758 - acc: 0.6152     \n",
      "Epoch 19/100\n",
      "5000/5000 [==============================] - 0s - loss: 1.0376 - acc: 0.6252     \n",
      "Epoch 20/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.9925 - acc: 0.6414     \n",
      "Epoch 21/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.9341 - acc: 0.6600     \n",
      "Epoch 22/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.9151 - acc: 0.6688     \n",
      "Epoch 23/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.8546 - acc: 0.6918     \n",
      "Epoch 24/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.8070 - acc: 0.7152     \n",
      "Epoch 25/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.7621 - acc: 0.7308     \n",
      "Epoch 26/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.7290 - acc: 0.7414     \n",
      "Epoch 27/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.6860 - acc: 0.7574     \n",
      "Epoch 28/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.6414 - acc: 0.7760     \n",
      "Epoch 29/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.5918 - acc: 0.7898     \n",
      "Epoch 30/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.5770 - acc: 0.7926     \n",
      "Epoch 31/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.5320 - acc: 0.8114     \n",
      "Epoch 32/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.5060 - acc: 0.8190     \n",
      "Epoch 33/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.4778 - acc: 0.8328     \n",
      "Epoch 34/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.4391 - acc: 0.8422     \n",
      "Epoch 35/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.4319 - acc: 0.8430     \n",
      "Epoch 36/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.4076 - acc: 0.8558     \n",
      "Epoch 37/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.3729 - acc: 0.8662     \n",
      "Epoch 38/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.3830 - acc: 0.8650     \n",
      "Epoch 39/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.3668 - acc: 0.8738     \n",
      "Epoch 40/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.3328 - acc: 0.8792     \n",
      "Epoch 41/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.3121 - acc: 0.8926     \n",
      "Epoch 42/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2968 - acc: 0.8956     \n",
      "Epoch 43/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2820 - acc: 0.9004     \n",
      "Epoch 44/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2702 - acc: 0.9082     \n",
      "Epoch 45/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2594 - acc: 0.9074     \n",
      "Epoch 46/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2693 - acc: 0.9042     \n",
      "Epoch 47/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2538 - acc: 0.9088     \n",
      "Epoch 48/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2431 - acc: 0.9124     \n",
      "Epoch 49/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2132 - acc: 0.9246     \n",
      "Epoch 50/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2163 - acc: 0.9200     \n",
      "Epoch 51/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2086 - acc: 0.9278     \n",
      "Epoch 52/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2127 - acc: 0.9248     \n",
      "Epoch 53/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1910 - acc: 0.9356     \n",
      "Epoch 54/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.2001 - acc: 0.9362     \n",
      "Epoch 55/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1779 - acc: 0.9388     \n",
      "Epoch 56/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1832 - acc: 0.9358     \n",
      "Epoch 57/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1807 - acc: 0.9354     \n",
      "Epoch 58/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1647 - acc: 0.9418     \n",
      "Epoch 59/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1764 - acc: 0.9374     \n",
      "Epoch 60/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1613 - acc: 0.9460     \n",
      "Epoch 61/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1723 - acc: 0.9424     \n",
      "Epoch 62/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1613 - acc: 0.9426     \n",
      "Epoch 63/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1538 - acc: 0.9440     \n",
      "Epoch 64/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1716 - acc: 0.9430     \n",
      "Epoch 65/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1580 - acc: 0.9472     \n",
      "Epoch 66/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1669 - acc: 0.9382     \n",
      "Epoch 67/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1232 - acc: 0.9612     \n",
      "Epoch 68/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1460 - acc: 0.9502     \n",
      "Epoch 69/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1438 - acc: 0.9518     \n",
      "Epoch 70/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1291 - acc: 0.9542     \n",
      "Epoch 71/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1404 - acc: 0.9558     \n",
      "Epoch 72/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1142 - acc: 0.9628     \n",
      "Epoch 73/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1344 - acc: 0.9538     \n",
      "Epoch 74/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1623 - acc: 0.9418     \n",
      "Epoch 75/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1279 - acc: 0.9564     \n",
      "Epoch 76/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1358 - acc: 0.9524     \n",
      "Epoch 77/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1188 - acc: 0.9604     \n",
      "Epoch 78/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.0996 - acc: 0.9664     \n",
      "Epoch 79/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1297 - acc: 0.9584     \n",
      "Epoch 80/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1325 - acc: 0.9530     \n",
      "Epoch 81/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1161 - acc: 0.9612     \n",
      "Epoch 82/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1020 - acc: 0.9684     \n",
      "Epoch 83/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1240 - acc: 0.9598     \n",
      "Epoch 84/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1241 - acc: 0.9558     \n",
      "Epoch 85/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1089 - acc: 0.9614     \n",
      "Epoch 86/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1226 - acc: 0.9586     \n",
      "Epoch 87/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1111 - acc: 0.9628     \n",
      "Epoch 88/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1126 - acc: 0.9640     \n",
      "Epoch 89/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.0957 - acc: 0.9666     \n",
      "Epoch 90/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1281 - acc: 0.9546     \n",
      "Epoch 91/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1114 - acc: 0.9640     \n",
      "Epoch 92/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1049 - acc: 0.9624     \n",
      "Epoch 93/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.0954 - acc: 0.9674     \n",
      "Epoch 94/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1222 - acc: 0.9560     \n",
      "Epoch 95/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1030 - acc: 0.9672     \n",
      "Epoch 96/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1188 - acc: 0.9596     \n",
      "Epoch 97/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1347 - acc: 0.9572     \n",
      "Epoch 98/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1232 - acc: 0.9584     \n",
      "Epoch 99/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1084 - acc: 0.9626     \n",
      "Epoch 100/100\n",
      "5000/5000 [==============================] - 0s - loss: 0.1013 - acc: 0.9656     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdfd051a748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train first model\n",
    "\n",
    "model.fit(X_train_label, Y_train_label,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "#           validation_set=(X_train_label, Y_train_label),\n",
    "#           validation_split=0.1,  # (X_train_label, Y_train_label),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "# Y_train_unlabel_prob = model.predict(X_train_unlabel, batch_size=5000)\n",
    "Y_train_unlabel_class = model.predict_classes(X_train_unlabel, batch_size=5000)\n",
    "Y_train_unlabel = np_utils.to_categorical(Y_train_unlabel_class, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add all unlabeled data to labelled data\n",
    "X_train_label_unlabel = np.concatenate((X_train_label, X_train_unlabel), axis=0)\n",
    "Y_train_label_unlabel = np.concatenate((Y_train_label, Y_train_unlabel), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2899 - acc: 0.9045 - val_loss: 0.0608 - val_acc: 0.9834\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2839 - acc: 0.9060 - val_loss: 0.0378 - val_acc: 0.9888\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2791 - acc: 0.9086 - val_loss: 0.0518 - val_acc: 0.9832\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2863 - acc: 0.9046 - val_loss: 0.0452 - val_acc: 0.9862\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2719 - acc: 0.9094 - val_loss: 0.0577 - val_acc: 0.9814\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2768 - acc: 0.9075 - val_loss: 0.0434 - val_acc: 0.9872\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2753 - acc: 0.9078 - val_loss: 0.0343 - val_acc: 0.9912\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 4s - loss: 0.2662 - acc: 0.9093 - val_loss: 0.0342 - val_acc: 0.9904\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 4s - loss: 0.2536 - acc: 0.9154 - val_loss: 0.0460 - val_acc: 0.9848\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2761 - acc: 0.9095 - val_loss: 0.0536 - val_acc: 0.9844\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2560 - acc: 0.9143 - val_loss: 0.0409 - val_acc: 0.9894\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2509 - acc: 0.9160 - val_loss: 0.0369 - val_acc: 0.9888\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2547 - acc: 0.9152 - val_loss: 0.0319 - val_acc: 0.9888\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2596 - acc: 0.9141 - val_loss: 0.0356 - val_acc: 0.9900\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2725 - acc: 0.9093 - val_loss: 0.0426 - val_acc: 0.9858\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2559 - acc: 0.9143 - val_loss: 0.0545 - val_acc: 0.9850\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2460 - acc: 0.9165 - val_loss: 0.0321 - val_acc: 0.9920\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2513 - acc: 0.9161 - val_loss: 0.0315 - val_acc: 0.9918\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2571 - acc: 0.9159 - val_loss: 0.0473 - val_acc: 0.9862\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.2525 - acc: 0.9156 - val_loss: 0.0431 - val_acc: 0.9852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe022ac0588>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_label_unlabel, Y_train_label_unlabel,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=20,\n",
    "          validation_data=(X_train_label, Y_train_label),\n",
    "#           validation_split=0.1,  # (X_train_label, Y_train_label),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = (METHOD\n",
    "    + '_filters-64w5-64w3-512'\n",
    "    + '_epo-100-100-20' # + str(nb_epoch)\n",
    "    + '_acc-0.9156'\n",
    "    + '_lab-acc-0.9852'\n",
    ")\n",
    "model.save(MODEL_FOLDER + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = pickle.load(open(TEST_DATA_FILE, 'rb'))  # dict\n",
    "X_test = np.array(test_data['data']).reshape((10000, 3, 32, 32)).astype('float32')\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "result = model.predict_classes(X_test, batch_size=10000)\n",
    "csv_content = list(zip(test_data['ID'], result.tolist()))\n",
    "np.savetxt(OUTPUT_FOLDER + model_name + \".csv\", csv_content, fmt=\"%i\", header=\"ID,class\", comments=\"\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
